{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajnishdascse/Machine_Learning/blob/gh-pages/Implementation_of_Pegasus_xsum_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this notebook , I have implemented the **Google Pegasus Summarization model**\n",
        "**Dataset** :` google/pegasus-xsum`"
      ],
      "metadata": {
        "id": "WMYoh3F8-10Z"
      },
      "id": "WMYoh3F8-10Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defination of Pegasus:** It is a pre-training model with extracted gap-sentences for abstravtive summarization. In simple, It is a sequence to sequence model which gives abstractive summarization model\n",
        "\n"
      ],
      "metadata": {
        "id": "9rKvzpboBEuT"
      },
      "id": "9rKvzpboBEuT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Installing the dependencies and libaries"
      ],
      "metadata": {
        "id": "jFq_as82_jCw"
      },
      "id": "jFq_as82_jCw"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "daac4f30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daac4f30",
        "outputId": "ad35ea48-fe8e-4176-b832-998ea8e4efbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyTorch\n",
            "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
            "Building wheels for collected packages: PyTorch\n",
            "  Building wheel for PyTorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for PyTorch\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for PyTorch\n",
            "Failed to build PyTorch\n",
            "Installing collected packages: PyTorch\n",
            "    Running setup.py install for PyTorch ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-j59lwon7/pytorch_4619e146c78845ccb9f49ba65c483fa9/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-j59lwon7/pytorch_4619e146c78845ccb9f49ba65c483fa9/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-bck3fgzk/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/PyTorch Check the logs for full command output.\u001b[0m\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install PyTorch\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Import dependencies from the libaries"
      ],
      "metadata": {
        "id": "EAMlTgOmBY8r"
      },
      "id": "EAMlTgOmBY8r"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7a7853df",
      "metadata": {
        "id": "7a7853df"
      },
      "outputs": [],
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Defining a device for viewing the decoding text"
      ],
      "metadata": {
        "id": "R1pHS_5DB0Jv"
      },
      "id": "R1pHS_5DB0Jv"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "59e5434c",
      "metadata": {
        "id": "59e5434c"
      },
      "outputs": [],
      "source": [
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Loading the **Tokenizer** and **Model**"
      ],
      "metadata": {
        "id": "3BnJGuhlB_bk"
      },
      "id": "3BnJGuhlB_bk"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c644c65e",
      "metadata": {
        "id": "c644c65e"
      },
      "outputs": [],
      "source": [
        "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
        "# tokenizer = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\").to(torch_device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Performing the **Abstractive Summarization**"
      ],
      "metadata": {
        "id": "jVdKpJZJCR5l"
      },
      "id": "jVdKpJZJCR5l"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7530912d",
      "metadata": {
        "id": "7530912d"
      },
      "outputs": [],
      "source": [
        "text =  [\n",
        "    \"\"\"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris.\n",
        "    Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington\n",
        "   Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York\n",
        "  City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at\n",
        "  the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters,\n",
        "  the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\"\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Creating the tokens & number representation of our text\n"
      ],
      "metadata": {
        "id": "nxaT9MpZCjm7"
      },
      "id": "nxaT9MpZCjm7"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "81c2b56e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81c2b56e",
        "outputId": "5a600ed3-2d0e-4f77-e346-1b1798fab2fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  139,  5998,   117, 56966,  7641,  4653,   108, 63292,  4954,   158,\n",
              "          3930,   108,   160,   109,   310,  3098,   130,   142, 13042,   121,\n",
              "         23761,   563,   108,   111,   109, 22246,  1557,   115,  3165,   107,\n",
              "          3096,  1217,   117,  2151,   108,  6542, 10557,  7641,   143, 27789,\n",
              "          4954,   158,   124,   276,   477,   107,  2348,   203,  1187,   108,\n",
              "           109, 37695,  6817, 23861,   109,  1741, 20551,   112,   460,   109,\n",
              "         22246,   729,   121,  4109,  1557,   115,   109,   278,   108,   114,\n",
              "          1560,   126,   886,   118,  6820,   231,   430,   109, 15528,  3671,\n",
              "           115,   351,   859,   672,   140,  1554,   115,  9844,   107,   168,\n",
              "           140,   109,   211,  1557,   112,  1111,   114,  3098,   113,  3251,\n",
              "          7641,   107,  5223,   112,   109,   663,   113,   114, 16071, 11676,\n",
              "           134,   109,   349,   113,   109,  5998,   115, 20636,   108,   126,\n",
              "           117,   239, 17916,   197,   109, 15528,  3671,   141, 27815,  7641,\n",
              "         23121,  4954,   250,   110, 64898, 48453,   108,   109, 37695,  6817,\n",
              "           117,   109,   453, 22246,   294,   121, 11570,  1557,   115,  2481,\n",
              "           244,   109,  4315,  4380, 15258, 22947,   107,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokens = tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
        "tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Summarize the text and showing the output "
      ],
      "metadata": {
        "id": "ZWyuXrbCEA95"
      },
      "id": "ZWyuXrbCEA95"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b22629c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b22629c6",
        "outputId": "056a7274-1a10-4765-ed92-e4052a3afbb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    0,   139, 37695,  6817,   117,   114, 11023,   115,  3165,   108,\n",
              "         2481,   107,     1])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "summary = model.generate(**tokens)\n",
        "\n",
        "#Output of the summary tokens\n",
        "summary[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8) Decoding the summary"
      ],
      "metadata": {
        "id": "QmZ7bqUIEmtZ"
      },
      "id": "QmZ7bqUIEmtZ"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(summary[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jnoZ9_PGFAhN",
        "outputId": "ac7ba13d-88df-4ee7-e4fb-bce18e8cc1af"
      },
      "id": "jnoZ9_PGFAhN",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Eiffel Tower is a landmark in Paris, France.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "                              ----XXX-----"
      ],
      "metadata": {
        "id": "W4-Eojj-FsmE"
      },
      "id": "W4-Eojj-FsmE"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Implementation of Pegasus-xsum dataset",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}